{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 14:43:17,617\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-06-15 14:43:17,767\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/config.json from cache at /users/jmperez/.cache/huggingface/transformers/c26252806565e705085b65f69d7d544c05112fee06744845d6c067efcb278fff.31fdd4298ba667598119e493f82afb18fcd41f96366700ec7d6460c17c421feb\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/config.json from cache at /users/jmperez/.cache/huggingface/transformers/c26252806565e705085b65f69d7d544c05112fee06744845d6c067efcb278fff.31fdd4298ba667598119e493f82afb18fcd41f96366700ec7d6460c17c421feb\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/vocab.json from cache at /users/jmperez/.cache/huggingface/transformers/6226ecb69473d2e3b9b922e36a65f2ee47b07ada67d7df2ba9bf80223d0edf5d.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/merges.txt from cache at /users/jmperez/.cache/huggingface/transformers/b80363df6b4ebfb87db413189e3cfa091f97bca51c9d2e8374ee6dfc5a67e510.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/special_tokens_map.json from cache at /users/jmperez/.cache/huggingface/transformers/601312a9cb96656475ff2ef71b3b002f803e0889279718ab471aed2c84b95b18.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/config.json from cache at /users/jmperez/.cache/huggingface/transformers/c26252806565e705085b65f69d7d544c05112fee06744845d6c067efcb278fff.31fdd4298ba667598119e493f82afb18fcd41f96366700ec7d6460c17c421feb\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/config.json from cache at /users/jmperez/.cache/huggingface/transformers/c26252806565e705085b65f69d7d544c05112fee06744845d6c067efcb278fff.31fdd4298ba667598119e493f82afb18fcd41f96366700ec7d6460c17c421feb\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/pytorch_model.bin from cache at /users/jmperez/.cache/huggingface/transformers/a4d71e7c40e6dc9e9af1333021bcc5cef014cafefa7ee71bc819d43671663137.0c0364bddcd0cf04d5ac75c6dad75bde326aa9adcbd172a5d2a132789d16514a\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "2024-06-15 14:43:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf650527521441aa990ac6689b51ecac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 14:43:29 INFO: Downloaded file to /users/jmperez/stanza_resources/resources.json\n",
      "INFO:stanza:Downloaded file to /users/jmperez/stanza_resources/resources.json\n",
      "2024-06-15 14:43:29 WARNING: Language en package default expects mwt, which has been added\n",
      "WARNING:stanza:Language en package default expects mwt, which has been added\n",
      "2024-06-15 14:43:29 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "2024-06-15 14:43:29 INFO: Using device: cuda\n",
      "INFO:stanza:Using device: cuda\n",
      "2024-06-15 14:43:29 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2024-06-15 14:43:29 INFO: Loading: mwt\n",
      "INFO:stanza:Loading: mwt\n",
      "2024-06-15 14:43:29 INFO: Loading: sentiment\n",
      "INFO:stanza:Loading: sentiment\n",
      "2024-06-15 14:43:30 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pysentimiento import create_analyzer\n",
    "from textblob import TextBlob\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import tweetnlp\n",
    "import stanza\n",
    "from tqdm.auto import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "benchmark_datasets = {\n",
    "    \"sentiment\": [\"stanfordnlp/sst2\", \"takala/financial_phrasebank\"]\n",
    "}\n",
    "\n",
    "#pysentimient + tweetnlp + stanza\n",
    "analyzer = create_analyzer(\"sentiment\", lang=\"en\")\n",
    "model = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()`\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)\n",
    "\n",
    "def pysentimiento_analyzer(dataset):\n",
    "    id2label = dataset.features[\"label\"].names\n",
    "\n",
    "    outs = analyzer.predict(dataset[\"sentence\"])\n",
    "\n",
    "    if len(id2label) == 2:\n",
    "        # Only positive/negative\n",
    "        return [\"negative\" if x.probas[\"NEG\"] > x.probas[\"POS\"] else \"positive\" for x in outs]\n",
    "    else:\n",
    "        translation = {\"NEU\": \"neutral\", \"POS\": \"positive\", \"NEG\": \"negative\"}\n",
    "        return [translation[x.output] for x in outs]\n",
    "\n",
    "def stanza_analyzer(dataset):\n",
    "    id2label = dataset.features[\"label\"].names\n",
    "    outs = nlp(dataset[\"sentence\"])\n",
    "\n",
    "    def _get_sentiment(x):\n",
    "        if x.sentiment == 0:\n",
    "            return \"negative\"\n",
    "        elif x.sentiment == 2:\n",
    "            return \"positive\"\n",
    "        elif len(id2label) == 2:\n",
    "            # Flip a coin\n",
    "            if random.random() > 0.5:\n",
    "                return \"positive\"\n",
    "            else:\n",
    "                return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    return [_get_sentiment(x) for x in outs.sentences]\n",
    "\n",
    "def tweetnlp_analyzer(dataset):\n",
    "    id2label = dataset.features[\"label\"].names\n",
    "    outs = model.predict(dataset[\"sentence\"])\n",
    "    def get_tweetnlp_sentiment(x):\n",
    "        if x[\"label\"] in {\"positive\", \"negative\"}:\n",
    "            return x[\"label\"]\n",
    "        elif len(id2label) == 2:\n",
    "            # Flip a coin\n",
    "            if random.random() > 0.5:\n",
    "                return \"positive\"\n",
    "            else:\n",
    "                return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    return [get_tweetnlp_sentiment(x) for x in outs]\n",
    "\n",
    "def textblob_analyzer(dataset, threshold=0.1):\n",
    "    id2label = dataset.features[\"label\"].names\n",
    "    outs = [TextBlob(x).sentiment.polarity for x in dataset[\"sentence\"]]\n",
    "\n",
    "    def get_textblob_sentiment(x):\n",
    "        if len(id2label) == 2:\n",
    "            if x > 0:\n",
    "                return \"positive\"\n",
    "            else:\n",
    "                return \"negative\"\n",
    "        else:\n",
    "            if x > threshold:\n",
    "                return \"positive\"\n",
    "            elif x < -threshold:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"neutral\"\n",
    "\n",
    "    return [get_textblob_sentiment(x) for x in outs]\n",
    "\n",
    "def vader_analyzer(dataset):\n",
    "    id2label = dataset.features[\"label\"].names\n",
    "    outs = [vader.polarity_scores(x) for x in dataset[\"sentence\"]]\n",
    "\n",
    "    def get_vader_sentiment(x):\n",
    "        if len(id2label) == 2:\n",
    "            if x[\"pos\"] > x[\"neg\"]:\n",
    "                return \"positive\"\n",
    "            else:\n",
    "                return \"negative\"\n",
    "        else:\n",
    "            labels = [\"neg\", \"neu\", \"pos\"]\n",
    "\n",
    "            # get argmax\n",
    "            max_sent = max(range(len(labels)), key=lambda i: x[labels[i]])\n",
    "\n",
    "            return id2label[max_sent]\n",
    "    return [get_vader_sentiment(x) for x in outs]\n",
    "\n",
    "analyzers = {\n",
    "    \"pysentimiento\": pysentimiento_analyzer,\n",
    "    \"tweetnlp\": tweetnlp_analyzer,\n",
    "    \"stanza\": stanza_analyzer,\n",
    "    \"textblob\": textblob_analyzer,\n",
    "    \"vader\": vader_analyzer\n",
    "}\n",
    "\n",
    "ds = load_dataset(benchmark_datasets[\"sentiment\"][0])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbfa7e467184388ac61d9606891ab60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "preds = {\n",
    "    k: analyzer(ds[\"validation\"]) for k, analyzer in analyzers.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pysentimiento\n",
      "tweetnlp\n",
      "stanza\n",
      "textblob\n",
      "vader\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative F1</th>\n",
       "      <th>Positive F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pysentimiento</th>\n",
       "      <td>87.573964</td>\n",
       "      <td>88.320356</td>\n",
       "      <td>87.947160</td>\n",
       "      <td>87.990882</td>\n",
       "      <td>87.931506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stanza</th>\n",
       "      <td>83.274021</td>\n",
       "      <td>84.350721</td>\n",
       "      <td>83.812371</td>\n",
       "      <td>83.864649</td>\n",
       "      <td>83.797466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetnlp</th>\n",
       "      <td>81.573034</td>\n",
       "      <td>80.796253</td>\n",
       "      <td>81.184643</td>\n",
       "      <td>81.358885</td>\n",
       "      <td>81.257893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader</th>\n",
       "      <td>64.179104</td>\n",
       "      <td>69.361702</td>\n",
       "      <td>66.770403</td>\n",
       "      <td>67.171414</td>\n",
       "      <td>66.851899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob</th>\n",
       "      <td>62.610900</td>\n",
       "      <td>69.109948</td>\n",
       "      <td>65.860424</td>\n",
       "      <td>66.500154</td>\n",
       "      <td>66.017302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Negative F1  Positive F1   Macro F1  Macro Precision  \\\n",
       "pysentimiento    87.573964    88.320356  87.947160        87.990882   \n",
       "stanza           83.274021    84.350721  83.812371        83.864649   \n",
       "tweetnlp         81.573034    80.796253  81.184643        81.358885   \n",
       "vader            64.179104    69.361702  66.770403        67.171414   \n",
       "textblob         62.610900    69.109948  65.860424        66.500154   \n",
       "\n",
       "               Macro Recall  \n",
       "pysentimiento     87.931506  \n",
       "stanza            83.797466  \n",
       "tweetnlp          81.257893  \n",
       "vader             66.851899  \n",
       "textblob          66.017302  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "id2label = [\"negative\", \"positive\"]\n",
    "label2id = {label: i for i, label in enumerate(id2label)}\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, pred in preds.items():\n",
    "    print(name)\n",
    "    true_labels = ds[\"validation\"][\"label\"]\n",
    "    pred_labels = [label2id[x] for x in pred]\n",
    "\n",
    "    ret = classification_report(true_labels, pred_labels, target_names=id2label, output_dict=True)\n",
    "\n",
    "    res = {\n",
    "        \"Negative F1\": ret[\"negative\"][\"f1-score\"],\n",
    "        \"Positive F1\": ret[\"positive\"][\"f1-score\"],\n",
    "        \"Macro F1\": ret[\"macro avg\"][\"f1-score\"],\n",
    "        \"Macro Precision\": ret[\"macro avg\"][\"precision\"],\n",
    "        \"Macro Recall\": ret[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "    results[name] = res\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "# Sort by \"Macro F1\"\n",
    "df = df.sort_values(\"Macro F1\", ascending=False)\n",
    "\n",
    "df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Financial Phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 4217\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ed3cc0876d4608ac1cb75c46b4fb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4217\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds = load_dataset(\"takala/financial_phrasebank\", \"sentences_66agree\")\n",
    "print(ds)\n",
    "preds = {\n",
    "    k: analyzer(ds[\"train\"]) for k, analyzer in analyzers.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pysentimiento\n",
      "tweetnlp\n",
      "stanza\n",
      "textblob\n",
      "vader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative F1</th>\n",
       "      <th>Positive F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pysentimiento</th>\n",
       "      <td>0.637441</td>\n",
       "      <td>0.609615</td>\n",
       "      <td>0.682352</td>\n",
       "      <td>0.750389</td>\n",
       "      <td>0.645194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetnlp</th>\n",
       "      <td>0.513834</td>\n",
       "      <td>0.466513</td>\n",
       "      <td>0.588266</td>\n",
       "      <td>0.732070</td>\n",
       "      <td>0.548265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stanza</th>\n",
       "      <td>0.313076</td>\n",
       "      <td>0.324654</td>\n",
       "      <td>0.441454</td>\n",
       "      <td>0.463273</td>\n",
       "      <td>0.444095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob</th>\n",
       "      <td>0.289238</td>\n",
       "      <td>0.377609</td>\n",
       "      <td>0.448512</td>\n",
       "      <td>0.466919</td>\n",
       "      <td>0.439714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.251322</td>\n",
       "      <td>0.333713</td>\n",
       "      <td>0.333510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Negative F1  Positive F1  Macro F1  Macro Precision  \\\n",
       "pysentimiento     0.637441     0.609615  0.682352         0.750389   \n",
       "tweetnlp          0.513834     0.466513  0.588266         0.732070   \n",
       "stanza            0.313076     0.324654  0.441454         0.463273   \n",
       "textblob          0.289238     0.377609  0.448512         0.466919   \n",
       "vader             0.000000     0.003410  0.251322         0.333713   \n",
       "\n",
       "               Macro Recall  \n",
       "pysentimiento      0.645194  \n",
       "tweetnlp           0.548265  \n",
       "stanza             0.444095  \n",
       "textblob           0.439714  \n",
       "vader              0.333510  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "results = {}\n",
    "id2label = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(id2label)}\n",
    "for name, pred in preds.items():\n",
    "    print(name)\n",
    "    true_labels = ds[\"train\"][\"label\"]\n",
    "    pred_labels = [label2id[x] for x in pred]\n",
    "\n",
    "    ret = classification_report(true_labels, pred_labels, target_names=id2label, output_dict=True)\n",
    "\n",
    "    res = {\n",
    "        \"Negative F1\": ret[\"negative\"][\"f1-score\"],\n",
    "        \"Positive F1\": ret[\"positive\"][\"f1-score\"],\n",
    "        \"Macro F1\": ret[\"macro avg\"][\"f1-score\"],\n",
    "        \"Macro Precision\": ret[\"macro avg\"][\"precision\"],\n",
    "        \"Macro Recall\": ret[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "    results[name] = res\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df.sort_values(\"Macro F1\", ascending=False) * 100\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
