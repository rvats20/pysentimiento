{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 03:09:52,733\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-06-15 03:09:52,881\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac43fb5f7b0847959cad6e80f2e0dcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088f3fa929d04f268eddf16576461045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517d0278c37a4d2d84b21fd0cb8fff5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pysentimiento import create_analyzer\n",
    "from textblob import TextBlob\n",
    "from datasets import load_dataset\n",
    "import tweetnlp\n",
    "\n",
    "model = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()`\n",
    "\n",
    "\n",
    "\n",
    "benchmark_datasets = {\n",
    "    \"sentiment\": [\"stanfordnlp/sst2\", \"takala/financial_phrasebank\"]\n",
    "}\n",
    "\n",
    "analyzer = create_analyzer(\"sentiment\", lang=\"en\")\n",
    "\n",
    "\n",
    "ds = load_dataset(benchmark_datasets[\"sentiment\"][0])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': Value(dtype='int32', id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6af3e98dbc44d389875a5f91575bfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /users/jmperez/stanza_resources/resources.json\n",
      "WARNING:stanza:Language en package default expects mwt, which has been added\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "INFO:stanza:Using device: cuda\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: sentiment\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d5633f85d14f4eb48fe4326438f12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f503e3db89e34a1cbaa9e7ea90b6e054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0881668b20e54eddbd2800f42f30cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "pysentimiento_outs = analyzer.predict(ds[\"validation\"][\"sentence\"])\n",
    "textblob_outs = [TextBlob(x).sentiment.polarity for x in tqdm(ds[\"validation\"][\"sentence\"])]\n",
    "vader_outs = [vader.polarity_scores(x) for x in tqdm(ds[\"validation\"][\"sentence\"])]\n",
    "stanza_outs = nlp(ds[\"validation\"][\"sentence\"])\n",
    "\n",
    "tweetnlp_outs = model.predict(ds[\"validation\"][\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'positive'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetnlp_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "preds = {}\n",
    "\n",
    "# Get NEG or POS --ignore NEU\n",
    "preds[\"pysentimiento\"] = [\"negative\" if x.probas[\"NEG\"] > x.probas[\"POS\"] else \"positive\" for x in pysentimiento_outs]\n",
    "\n",
    "preds[\"textblob\"] = [\"negative\" if x < 0 else \"positive\" for x in textblob_outs]\n",
    "\n",
    "preds[\"vader\"] = [\"negative\" if x[\"neg\"] > x[\"pos\"] else \"positive\" for x in vader_outs]\n",
    "\n",
    "def get_stanza_sentiment(x):\n",
    "    if x.sentiment == 0:\n",
    "        return \"negative\"\n",
    "    elif x.sentiment == 2:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        # Flip a coin\n",
    "        if random.random() > 0.5:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "\n",
    "preds[\"stanza\"] = [get_stanza_sentiment(x) for x in stanza_outs.sentences]\n",
    "\n",
    "def get_tweetnlp_sentiment(x):\n",
    "\n",
    "    if x[\"label\"] in {\"positive\", \"negative\"}:\n",
    "        return x[\"label\"]\n",
    "    else:\n",
    "        # Flip a coin\n",
    "        if random.random() > 0.5:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "\n",
    "preds[\"tweetnlp\"] = [get_tweetnlp_sentiment(x) for x in tweetnlp_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pysentimiento\n",
      "textblob\n",
      "vader\n",
      "stanza\n",
      "tweetnlp\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "id2label = [\"negative\", \"positive\"]\n",
    "label2id = {label: i for i, label in enumerate(id2label)}\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, pred in preds.items():\n",
    "    print(name)\n",
    "    true_labels = ds[\"validation\"][\"label\"]\n",
    "    pred_labels = [label2id[x] for x in pred]\n",
    "\n",
    "    ret = classification_report(true_labels, pred_labels, target_names=id2label, output_dict=True)\n",
    "\n",
    "    res = {\n",
    "        \"Negative F1\": ret[\"negative\"][\"f1-score\"],\n",
    "        \"Positive F1\": ret[\"positive\"][\"f1-score\"],\n",
    "        \"Macro F1\": ret[\"macro avg\"][\"f1-score\"],\n",
    "        \"Macro Precision\": ret[\"macro avg\"][\"precision\"],\n",
    "        \"Macro Recall\": ret[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "    results[name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative F1</th>\n",
       "      <th>Positive F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pysentimiento</th>\n",
       "      <td>87.573964</td>\n",
       "      <td>88.320356</td>\n",
       "      <td>87.947160</td>\n",
       "      <td>87.990882</td>\n",
       "      <td>87.931506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob</th>\n",
       "      <td>50.909091</td>\n",
       "      <td>70.110701</td>\n",
       "      <td>60.509896</td>\n",
       "      <td>65.894397</td>\n",
       "      <td>62.418961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader</th>\n",
       "      <td>51.885370</td>\n",
       "      <td>70.490287</td>\n",
       "      <td>61.187828</td>\n",
       "      <td>66.501553</td>\n",
       "      <td>62.998863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stanza</th>\n",
       "      <td>84.976526</td>\n",
       "      <td>85.650224</td>\n",
       "      <td>85.313375</td>\n",
       "      <td>85.322608</td>\n",
       "      <td>85.307738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetnlp</th>\n",
       "      <td>80.590238</td>\n",
       "      <td>80.185400</td>\n",
       "      <td>80.387819</td>\n",
       "      <td>80.472006</td>\n",
       "      <td>80.435927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Negative F1  Positive F1   Macro F1  Macro Precision  \\\n",
       "pysentimiento    87.573964    88.320356  87.947160        87.990882   \n",
       "textblob         50.909091    70.110701  60.509896        65.894397   \n",
       "vader            51.885370    70.490287  61.187828        66.501553   \n",
       "stanza           84.976526    85.650224  85.313375        85.322608   \n",
       "tweetnlp         80.590238    80.185400  80.387819        80.472006   \n",
       "\n",
       "               Macro Recall  \n",
       "pysentimiento     87.931506  \n",
       "textblob          62.418961  \n",
       "vader             62.998863  \n",
       "stanza            85.307738  \n",
       "tweetnlp          80.435927  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Financial Phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 4217\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds = load_dataset(\"takala/financial_phrasebank\", \"sentences_66agree\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['negative', 'neutral', 'positive'],\n",
       " {'negative': 0, 'neutral': 1, 'positive': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(id2label)}\n",
    "\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3614a5408d4f6fad91c8b7b09e2461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4217\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad9d388a4ae48ca90deaad93176c812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be96a12e2ff346cca311acf8c1fbd57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pysentimiento_outs = analyzer.predict(ds[\"train\"][\"sentence\"])\n",
    "textblob_outs = [TextBlob(x).sentiment.polarity for x in tqdm(ds[\"train\"][\"sentence\"])]\n",
    "vader_outs = [vader.polarity_scores(x) for x in tqdm(ds[\"train\"][\"sentence\"])]\n",
    "stanza_outs = nlp(ds[\"train\"][\"sentence\"])\n",
    "tweetnlp_outs = model.predict(ds[\"train\"][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "preds = {}\n",
    "\n",
    "# Get NEG or POS --ignore NEU\n",
    "pysent_mask = {\"NEG\": \"negative\", \"POS\": \"positive\", \"NEU\": \"neutral\"}\n",
    "\n",
    "preds[\"pysentimiento\"] = [pysent_mask[x.output] for x in pysentimiento_outs]\n",
    "\n",
    "def get_textblob_sentiment(x, neutral_threshold=0.1):\n",
    "    if x < -neutral_threshold:\n",
    "        return \"negative\"\n",
    "    elif x > neutral_threshold:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "preds[\"textblob\"] = [get_textblob_sentiment(x) for x in textblob_outs]\n",
    "\n",
    "def get_vader_sentiment(x):\n",
    "    sents = [\"neg\", \"neu\", \"pos\"]\n",
    "\n",
    "    # get argmax\n",
    "    max_sent = max(range(len(sents)), key=lambda i: x[sents[i]])\n",
    "\n",
    "    return id2label[max_sent]\n",
    "\n",
    "preds[\"vader\"] = [get_vader_sentiment(x) for x in vader_outs]\n",
    "\n",
    "\n",
    "preds[\"stanza\"] = [id2label[x.sentiment] for x in stanza_outs.sentences]\n",
    "\n",
    "preds[\"tweetnlp\"] = [x[\"label\"] for x in tweetnlp_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pysentimiento\n",
      "textblob\n",
      "vader\n",
      "stanza\n",
      "tweetnlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, pred in preds.items():\n",
    "    print(name)\n",
    "    true_labels = ds[\"train\"][\"label\"]\n",
    "    pred_labels = [label2id[x] for x in pred]\n",
    "\n",
    "    ret = classification_report(true_labels, pred_labels, target_names=id2label, output_dict=True)\n",
    "\n",
    "    res = {\n",
    "        \"Negative F1\": ret[\"negative\"][\"f1-score\"],\n",
    "        \"Positive F1\": ret[\"positive\"][\"f1-score\"],\n",
    "        \"Macro F1\": ret[\"macro avg\"][\"f1-score\"],\n",
    "        \"Macro Precision\": ret[\"macro avg\"][\"precision\"],\n",
    "        \"Macro Recall\": ret[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "    results[name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative F1</th>\n",
       "      <th>Positive F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pysentimiento</th>\n",
       "      <td>0.637441</td>\n",
       "      <td>0.609615</td>\n",
       "      <td>0.682352</td>\n",
       "      <td>0.750389</td>\n",
       "      <td>0.645194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob</th>\n",
       "      <td>0.289238</td>\n",
       "      <td>0.377609</td>\n",
       "      <td>0.448512</td>\n",
       "      <td>0.466919</td>\n",
       "      <td>0.439714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.251322</td>\n",
       "      <td>0.333713</td>\n",
       "      <td>0.333510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stanza</th>\n",
       "      <td>0.313076</td>\n",
       "      <td>0.324654</td>\n",
       "      <td>0.441454</td>\n",
       "      <td>0.463273</td>\n",
       "      <td>0.444095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetnlp</th>\n",
       "      <td>0.513834</td>\n",
       "      <td>0.466513</td>\n",
       "      <td>0.588266</td>\n",
       "      <td>0.732070</td>\n",
       "      <td>0.548265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Negative F1  Positive F1  Macro F1  Macro Precision  \\\n",
       "pysentimiento     0.637441     0.609615  0.682352         0.750389   \n",
       "textblob          0.289238     0.377609  0.448512         0.466919   \n",
       "vader             0.000000     0.003410  0.251322         0.333713   \n",
       "stanza            0.313076     0.324654  0.441454         0.463273   \n",
       "tweetnlp          0.513834     0.466513  0.588266         0.732070   \n",
       "\n",
       "               Macro Recall  \n",
       "pysentimiento      0.645194  \n",
       "textblob           0.439714  \n",
       "vader              0.333510  \n",
       "stanza             0.444095  \n",
       "tweetnlp           0.548265  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
